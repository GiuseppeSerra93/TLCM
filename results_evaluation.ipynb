{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    " *     THIS FILE BELONGS TO THE PROGRAM: TLCM \n",
    " *\n",
    " *        File: results.evaluation.ipynb\n",
    " *\n",
    " *     Authors: Deleted for purposes of anonymity \n",
    " *\n",
    " *     Proprietor: Deleted for purposes of anonymity --- PROPRIETARY INFORMATION\n",
    " * \n",
    " * The software and its source code contain valuable trade secrets and shall be maintained in\n",
    " * confidence and treated as confidential information. The software may only be used for \n",
    " * evaluation and/or testing purposes, unless otherwise explicitly stated in the terms of a\n",
    " * license agreement or nondisclosure agreement with the proprietor of the software. \n",
    " * Any unauthorized publication, transfer to third parties, or duplication of the object or\n",
    " * source code---either totally or in part---is strictly prohibited.\n",
    " *\n",
    " *     Copyright (c) 2021 Proprietor: Deleted for purposes of anonymity\n",
    " *     All Rights Reserved.\n",
    " *\n",
    " * THE PROPRIETOR DISCLAIMS ALL WARRANTIES, EITHER EXPRESS OR \n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY \n",
    " * AND FITNESS FOR A PARTICULAR PURPOSE AND THE WARRANTY AGAINST LATENT \n",
    " * DEFECTS, WITH RESPECT TO THE PROGRAM AND ANY ACCOMPANYING DOCUMENTATION. \n",
    " * \n",
    " * NO LIABILITY FOR CONSEQUENTIAL DAMAGES:\n",
    " * IN NO EVENT SHALL THE PROPRIETOR OR ANY OF ITS SUBSIDIARIES BE \n",
    " * LIABLE FOR ANY DAMAGES WHATSOEVER (INCLUDING, WITHOUT LIMITATION, DAMAGES\n",
    " * FOR LOSS OF BUSINESS PROFITS, BUSINESS INTERRUPTION, LOSS OF INFORMATION, OR\n",
    " * OTHER PECUNIARY LOSS AND INDIRECT, CONSEQUENTIAL, INCIDENTAL,\n",
    " * ECONOMIC OR PUNITIVE DAMAGES) ARISING OUT OF THE USE OF OR INABILITY\n",
    " * TO USE THIS PROGRAM, EVEN IF the proprietor HAS BEEN ADVISED OF\n",
    " * THE POSSIBILITY OF SUCH DAMAGES.\n",
    " * \n",
    " * For purposes of anonymity, the identity of the proprietor is not given herewith. \n",
    " * The identity of the proprietor will be given once the review of the \n",
    " * conference submission is completed. \n",
    " *\n",
    " * THIS HEADER MAY NOT BE EXTRACTED OR MODIFIED IN ANY WAY.\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_preprocessed_data = utils.DIR_PREPROCESSED_DATA\n",
    "dir_training_data = utils.DIR_TRAIN_TEST_DATA\n",
    "dir_results = utils.DIR_RESULTS\n",
    "K = utils.K\n",
    "L = utils.L\n",
    "V = 2000\n",
    "sigma_u = utils.SIGMA_USERS\n",
    "sigma_p = utils.SIGMA_PRODUCTS\n",
    "categories = utils.CATEGORIES\n",
    "\n",
    "def dist(xy, xy2):\n",
    "    x1, y1 = xy\n",
    "    x2, y2 = xy2\n",
    "    dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def neighborhood_function(z1, z, sigma=0.5):\n",
    "    output = []\n",
    "    num = sum([np.exp(-dist(z1,zz)/(2*sigma)) for zz in z])\n",
    "    for z2 in z:\n",
    "        den = np.exp(-dist(z1,z2)/(2*sigma))\n",
    "        output.append(den/num)\n",
    "    return output\n",
    "\n",
    "def latex_words(p_w_yx, inverse_kw_map, grid, num_words=10):\n",
    "    num_classes = len(grid)\n",
    "    grid_words = defaultdict(list)\n",
    "    output_words = []\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        top_k = [k[1] for k in sorted(zip(p_w_yx[i], inverse_kw_map.keys()), reverse=True)[:num_words]]\n",
    "        top_words = [inv_keywords_map[k] for k in top_k]\n",
    "        grid_words[grid[i][0]].append(top_words)\n",
    "\n",
    "    for key in grid_words.keys():\n",
    "        output_words.extend(np.array(grid_words[key]).T)\n",
    "        \n",
    "    return output_words\n",
    "\n",
    "grid_user = [(int(i), int(j)) for i in range(int(np.sqrt(K))) for j in range(int(np.sqrt(K)))]\n",
    "grid_prod = [(int(i), int(j)) for i in range(int(np.sqrt(L))) for j in range(int(np.sqrt(L)))]\n",
    "sum_l = [np.arange(L)+L*i for i in range(K)]\n",
    "sum_k = [np.arange(0, K*L, L)+1*i for i in range(L)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'automotive'\n",
    "dir_preprocessed_data_c = dir_preprocessed_data + category + '/'\n",
    "dir_results_c = dir_results + category\n",
    "dir_results_CNN_c = dir_results_c + '/results_CNN_{0}_{1}/'.format(K, L)\n",
    "dir_results_EM_c = dir_results_c + '/results_EM_{0}_{1}/'.format(K, L)\n",
    "dir_figures_c = dir_results_c + '/figures/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_dict = {}\n",
    "nll_train = pickle.load(open(dir_results_EM_c + 'nll_train.pkl', 'rb'))\n",
    "nll_test = pickle.load(open(dir_results_EM_c + 'nll_test.pkl', 'rb'))\n",
    "ticks = np.arange(0, len(nll_train) + 1, 5)\n",
    "plt.plot(nll_train, label='Train')\n",
    "plt.plot(nll_test, label='Test')\n",
    "plt.legend()\n",
    "plt.title(category, fontsize=14)\n",
    "plt.xticks(ticks, labels=[i for i in ticks])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_map = pickle.load(open(dir_preprocessed_data_c + 'users_map.pkl', 'rb'))  # {user_ID: idx_u}\n",
    "products_map = pickle.load(open(dir_preprocessed_data_c + 'products_map.pkl', 'rb')) # {prod_ID: idx_p}\n",
    "keywords_map = pickle.load(open(dir_preprocessed_data_c + 'keywords_map.pkl', 'rb')) # {word_ID: idx_w}\n",
    "users_test = pickle.load(open(dir_preprocessed_data_c + 'users_test.pkl', 'rb'))\n",
    "\n",
    "# word index from 0 to 2000, originally from 1 to 2001 (due to padding)\n",
    "# --> {k:v-1 for k,v in keywords_map.items()}\n",
    "keywords_map = {k:v-1 for k,v in keywords_map.items()} # {word: idx_w}\n",
    "inv_keywords_map = {v-1:k for k,v in keywords_map.items()} # {idx_w: word}\n",
    "inv_users_map = {v:k for k,v in users_map.items()} # {idx_u: user_ID}\n",
    "inv_products_map = {v:k for k,v in products_map.items()} # {idx_p: prod_ID}\n",
    "inv_keywords_map = {v:k for k,v in keywords_map.items()} # {idx_w: word_ID}\n",
    "\n",
    "statistics_dict[category] = {'# users':len(users_map), '# products':len(products_map)}\n",
    "print(pd.DataFrame(statistics_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_figures_c):  # create directory if it does not exist\n",
    "    print('\\tCreate new output directory:', dir_figures_c)\n",
    "    os.makedirs(dir_figures_c)\n",
    "\n",
    "p_w_yuyp = pickle.load(open(dir_results_EM_c + 'p_w_yuyp.pkl', 'rb'))\n",
    "p_w_yuyp_T = p_w_yuyp.T\n",
    "p_zu_u = pickle.load(open(dir_results_EM_c + 'p_zu_u.pkl', 'rb'))\n",
    "p_zp_p = pickle.load(open(dir_results_EM_c + 'p_zp_p.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative extension for unseen users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_test_dict = {}\n",
    "\n",
    "for review in users_test:\n",
    "    user_id = review[0]\n",
    "    product_id = review[1]\n",
    "    text = review[2]\n",
    "    if users_test_dict.get(user_id) == None:\n",
    "        users_test_dict[user_id] = [[product_id, text]]\n",
    "    else:\n",
    "        users_test_dict[user_id].append([product_id, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = list(users_test_dict.keys())[0]\n",
    "print('User ID: {0}'.format(user_id))\n",
    "for review in users_test_dict[user_id]:\n",
    "        print(review[0],[inv_keywords_map[w] for w in review[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_review = users_test_dict[user_id][5]\n",
    "product_test = products_map[sampled_review[0]]\n",
    "review_test = sampled_review[1]\n",
    "print(sampled_review)\n",
    "print([inv_keywords_map[w] for w in review_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_zp_prod = p_zp_p[product_test]\n",
    "p_zu_user = [1./K]*K             # flat prior (we do not have information about this user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P(w|\\mathbf{{y}}_{u}^{k'},\\bar p) = \\sum_{\\ell'} P(w|\\mathbf{{y}}_{u}^{k'},\\mathbf{{y}}_{p}^{\\ell'}) P(\\mathbf{{y}}_{p}^{\\ell'} |\\bar p) \\qquad \\forall w \\in \\bar r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w_yup = np.array([np.sum((np.multiply(p_w_yuyp_T[w][sum_l[i]],p_zp_prod))) \n",
    "                    for w in review_test for i in range(K)]).reshape(len(review_test),K)\n",
    "p_w_yup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P(\\bar{r}|\\mathbf{{y}}_{u}^{k'},\\bar p) = \\prod_{w \\in \\bar{r}}P(w|\\mathbf{{y}}_{u}^{k'},\\bar p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_yup = np.prod(p_w_yup, axis=0)\n",
    "p_r_yup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P(\\mathbf{{y}}_{u}^{k'}|\\bar{r}) = \\frac{P(\\bar{r}|\\mathbf{{y}}_{u}^{k'},\\bar p)P(\\mathbf{y}_u^{k^{\\prime}}|u^i)}\n",
    "                                    {\\sum_{k^{''}}P(\\bar{r}|\\mathbf{{y}}_{u}^{k^{''}},\\bar p)P(\\mathbf{y}_u^{k^{''}}|u^i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "den = np.sum(p_r_yup)    \n",
    "p_yu_r = p_r_yup/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(K), p_yu_r)\n",
    "plt.show()\n",
    "np.sum(p_yu_r) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_w_yu = np.array([sum(p_w_yuyp[sum_l[i]])/L for i in range(K)])\n",
    "p_w_yp = np.array([sum(p_w_yuyp[sum_k[i]])/K for i in range(L)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_u = []\n",
    "for i in range(K):\n",
    "    top_k = [k[1] for k in sorted(zip(p_w_yu[i], sorted(inv_keywords_map.keys())), reverse = True)[:10]]\n",
    "    top_words = [inv_keywords_map[k] for k in top_k]\n",
    "    labels_u.append('\\n'.join(top_words))\n",
    "    \n",
    "labels_u = np.array(labels_u).reshape(int(np.sqrt(K)),int(np.sqrt(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,13))\n",
    "sns.heatmap(p_yu_r.reshape(int(np.sqrt(K)),int(np.sqrt(K))), annot=labels_u, linewidths=0.3, fmt='', cmap=\"GnBu\",\n",
    "           annot_kws={\"fontsize\":13}, linecolor='black')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig(dir_figures_c + 'generative_extension.pdf', format='pdf', dpi=800, transparent=True, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print([inv_keywords_map[w] for w in review_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User latent class organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_u already computed\n",
    "values_u = np.ones(K)\n",
    "plt.figure(figsize=(10,13))\n",
    "sns.heatmap(values_u.reshape(int(np.sqrt(K)),int(np.sqrt(K))), annot=labels_u, linewidths=0.3, fmt='', cmap=\"rocket_r\",\n",
    "           annot_kws={\"fontsize\":13}, linecolor='black', cbar=False)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig(dir_figures_c + 'map_users.pdf', format='pdf', dpi=800, transparent=True, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product latent class organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p = []\n",
    "for i in range(L):\n",
    "    top_k = [k[1] for k in sorted(zip(p_w_yp[i], sorted(inv_keywords_map.keys())), reverse = True)[:10]]\n",
    "    top_words = [inv_keywords_map[k] for k in top_k]\n",
    "    labels_p.append('\\n'.join(top_words))\n",
    "    print(grid_prod[i], top_words)\n",
    "    print()\n",
    "    \n",
    "labels_p = np.array(labels_p).reshape(int(np.sqrt(L)),int(np.sqrt(L)))\n",
    "values_p = np.ones(L)\n",
    "plt.figure(figsize=(8,11))\n",
    "sns.heatmap(values_p.reshape(int(np.sqrt(L)),int(np.sqrt(L))), annot=labels_p, linewidths=0.3, fmt='', cmap=\"rocket_r\",\n",
    "           annot_kws={\"fontsize\":13}, linecolor='black', cbar=False)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.savefig(dir_figures_c + 'map_products.pdf', format='pdf', dpi=800, transparent=True, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
